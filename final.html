<html>
  <head>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default"></script>
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap"
      rel="stylesheet"
    />
    <style>
      h1 {
        text-align: center;
      }

      .container {
        margin: 0 auto;
        padding: 60px 20%;
      }

      figure {
        text-align: center;
      }

      img {
        display: inline-block;
      }

      body {
        font-family: "Inter", sans-serif;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <h1>SELFI3-D Milestone</h1>
      <div style="text-align: center">
        <b>Team Members</b>: Akshaan Ahuja, Jameson Crate, Michelle Chen,
        Valerie Li
      </div>

      <br />
      <div style="text-align: center; margin: 20px 0">
        <p><b>Project Links:</b></p>
        <p>
          Video Presentation:
          <a
            href="https://drive.google.com/file/d/1Eno7R0phGs-EzA_l5CGQLaA5SdsYrIKr/view?usp=sharing"
          >
            https://drive.google.com/file/d/1Eno7R0phGs-EzA_l5CGQLaA5SdsYrIKr/view?usp=sharing</a
          >
        </p>
        <p>
          Slides:
          <a
            href="https://docs.google.com/presentation/d/1a-stRxFNvy2kxD5SsM-ItsuNpEQqmf0-zQjJ58QCRrE/edit?usp=sharing"
            >https://docs.google.com/presentation/d/1a-stRxFNvy2kxD5SsM-ItsuNpEQqmf0-zQjJ58QCRrE/edit?usp=sharing</a
          >
        </p>
        <p>
          GitHub Repository:
          <a href="https://github.com/Jameson-Crate/SELFI3-D/"
            >https://github.com/Jameson-Crate/SELFI3-D/</a
          >
        </p>
        <p>
          Project Webpage:
          <a href="https://jameson-crate.github.io/SELFI3-D/milestone.html"
            >https://jameson-crate.github.io/SELFI3-D/milestone.html</a
          >
        </p>
      </div>

      <br />
      <h2>Abstract</h2>
      <p>
        SELFI3-D is a project focused on creating high-quality 3D face
        reconstructions from various views of a person, as well as applying
        custom texture mapping to the resulting geometry. Our goal is to develop
        a pipeline that can generate accurate 3D face models and enable creative
        applications on these models, such as virtual face tattoos. Our pipeline
        is broken down into a few key steps. Firstly, from a set of input views,
        we constructed a sparse point cloud using DUSt3R as well as a mesh
        constructed from the point cloud, providing us with a starting point for
        our 3D reconstruction. Next, we used MAST3R to produce a dense point
        cloud from the same set of input views, creating a dense 3D point cloud.
        We then used CloudCompare to remove outliers from the mesh and refine
        the geometry. On our meshes, we applied bump mapping to the surface of
        the geometry to give the appearance of natural skin texture. Finally, we
        applied a custom texture mapping to the resulting geometry, allowing us
        to add tattoos to the face.
      </p>

      <h2>Technical Approach</h2>
      <p>
        Our pipeline is broken down into a few key steps. Firstly, from a set of
        input views, we constructed a sparse point cloud using DUSt3R, providing
        us with a starting point for our 3D reconstruction. Next, we used MAST3R
        to produce a dense point cloud from the same set of input views,
        creating a dense 3D mesh. We then used CloudCompare to remove outliers
        from the mesh and refine the geometry. We then proceeded to apply bump mapping to the surface of the mesh, which gave the appearance of natural skin texture. 
        Finally, we applied a custom
        texture mapping to the resulting geometry, allowing us to apply custom
        textures, such as tattoos, to the face.

        <h3>DUSt3R, MAST3R</h3>
        <p>
          DUSt3R and MAST3R are both open-source tools for creating varying densities of 3D point clouds from a set of input views of varying angles. 
          We used DUSt3R to create a sparse point cloud from the input views, as well as a mesh from the point cloud, which we then used as a starting point for our 3D reconstruction. 
          We then used MAST3R to produce a dense point cloud from the same set of input views.
        </p>

        <h3>CloudCompare</h3>
        <p>
          CloudCompare is an open-source tool for creating and editing 3D point clouds. 
          We used CloudCompare to remove outliers from the mesh and refine the geometry.
        </p>

      </p>

      <h2>Results</h2>
      <p>blah blah blah</p>

      <h2>References</h2>
      <p>
        1. DUSt3R 2. MAST3R 3. CloudCompare: https://github.com/cloudcompare/cloudcompare 4. COLMAP 5. FaceScape 6. Open3D 7.
        Perlin Noise 8. Poisson Reconstruction: https://github.com/mkazhdan/PoissonRecon
      </p>

      <h2>Team Contributions</h2>
      <p>Valerie Li:</p>
      <ol>
        <li>Worked on creating a 3D mask overlay through FaceScape's
          bilinear model. This involved updating or refactoring outdated
          dependencies, fixing API breaks, and tuning model hyper-parameters to
          fit our dataset. </li>
        <li>Designed and implemented procedural bump mapping by
          displacing each mesh vertex along its normal using a 3D Perlin noise
          field. Initially, tried a 2D random noise approach, which produced
          streaks, instead of true, isotropic skin-like micro-detail, which the 3D
          Perlin noise field did!</li>
        <li>Developed and debugged per-vertex
          texture-mapping routines: created both orthographic and pinhole UV
          projection methods, handled coordinate transforms
          (reflect/rotate/translate), in an attempt to pick up skin-tone and hair
          coloration from a frontal photograph. However, this did not succeed, and
          the mesh did not change colors.</li>
          <li>
            Built an RGB overlay tattoo:
            composited an input of a transparent facial tattoo into a texture atlas,
            generated the corresponding .obj file with embedded UVs, and attempted
            to bake the overlay onto the mesh. Unfortunately, this also did not work
            on the mesh. Ultimately, unable to succeed in rendering the mesh in a
            different color.
          </li>
      </ol>
      </p>

      <p>Akshaan Ahuja:</p>
      <ol>
        <li>
         Worked on running DUST3R to create a sparse point cloud from the input views. 
         This involved debugging the output based on the produced confidence maps. 
         Experimented with varying the amount of input views as well as the difference in camera positions between views.</li>
        <li>
        Implemented Poisson Reconstruction to convert our dense point cloud from MAST3R into a complete mesh. 
        Initially attempted a novel Python implementation based on the logic from the state-of-the-art Poisson Reconstruction repository: https://github.com/mkazhdan/PoissonRecon, working through the mathematical foundations of the algorithm. 
        Later utilized CloudCompare software for a more robust solution, where I imported the point cloud, debugged and filtered noisy or outlying points, then applied Poisson Reconstruction. 
        This process required significant experimentation with reconstruction depth, by varying the Octree depth to achieve the best results. 
        After reconstruction, I trimmed the model based on the confidence values of the reconstruction to remove low-quality mesh regions, resulting in a high-fidelity 3D facial mesh that was ready for the next step of the pipeline which was bump mapping.

        In order to feed into CloudCompare, I created a script to estimate the normals of each point in our dense point cloud that was fed into CloudCompare. 
        </li>
      </ol>

      <td style="text-align: center">
        <img src="facescape.jpeg" width="400px" />
        <figcaption>
          The mask is overlayed on the right image. The mask was generated from
          first fitting the FaceScape model with 2-D facial landmarks from the
          input picture. Then, the resulting mesh was normalized and positioned
          in a straight on view. Finally, the depth map is overlayed onto the
          original image.
        </figcaption>
      </td>
    </div>
  </body>
</html>
